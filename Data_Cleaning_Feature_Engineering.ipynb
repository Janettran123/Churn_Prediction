{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Due to server space limitations, files were uploaded and separated based on customer information such as account creation, transactions, streaming activity, and website activity (adding to wishlist, deleting, rating content)\n",
        "\n",
        "Data ranges from 2016 - 2023 so the following code is to pull only 2023 data from each file\n",
        "\n",
        "Additionally, due to server space limitations, an entire file cannot by loaded into python, therefore we need to separate each file by year or import by chunks"
      ],
      "metadata": {
        "id": "oN5mYAyuFcxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JaUdTHDFQfx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "xqYPsUCaQodV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to pull from main server\n",
        "\n",
        "account_creation = pd.read_csv(\"../acc.csv\")\n",
        "transactions= pd.read_csv(\"../txns.csv\")\n",
        "ratings = pd.read_csv(\"../rate.csv\")\n",
        "avod = pd.read_csv(\"../avod.csv\")\n",
        "wishlist = pd.read_csv(\"../wish.csv\")"
      ],
      "metadata": {
        "id": "RgYp1FjVGAeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ACCOUNTS 2023\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 100000  # Adjust the chunk size as needed\n",
        "\n",
        "# Read the chunks of the CSV file\n",
        "account_chunks = pd.read_csv(\"../acc.csv\", header=0, chunksize=chunk_size)\n",
        "\n",
        "# Initialize an empty list to store filtered chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "# Loop through the chunks\n",
        "for chunk in account_chunks:\n",
        "    # Convert createtime to datetime\n",
        "    chunk['createtime'] = pd.to_datetime(chunk['createtime'])\n",
        "\n",
        "    # Filter for 2023\n",
        "    chunk_2023 = chunk.loc[chunk['createtime'].dt.year == 2023]\n",
        "\n",
        "    # Break up createtime into date and time columns\n",
        "    chunk_2023['Date'] = chunk_2023['createtime'].dt.date\n",
        "    chunk_2023['Time'] = chunk_2023['createtime'].dt.time\n",
        "\n",
        "    # Replace the createtime column with a simpler version\n",
        "    chunk_2023['date_created'] = pd.to_datetime(chunk_2023['Date'])\n",
        "    chunk_2023 = chunk_2023.drop(['Date', 'createtime'], axis=1)\n",
        "\n",
        "    # Create the month, year, day, and quarter columns\n",
        "    chunk_2023['year'] = chunk_2023['date_created'].dt.year\n",
        "    chunk_2023['month']\n"
      ],
      "metadata": {
        "id": "oDE08eGJk375"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILTER ALL TVOD FILES FOR 2023\n",
        "\n",
        "# Define the directory where the CSV files are located\n",
        "directory = \"../\"\n",
        "# Define the list of CSV files\n",
        "csv_files = ['tvod_1.csv', 'tvod_2.csv',\n",
        "             'tvod_3.csv', 'tvod_4.csv',\n",
        "             'tvod_5.csv', 'tvod_6.csv',\n",
        "             'tvod_7.csv']\n",
        "\n",
        "# Initialize an empty list to store filtered DataFrames\n",
        "filtered_dfs = []\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Read the CSV file in chunks\n",
        "    chunk_size = 100000\n",
        "    chunks = pd.read_csv(os.path.join(directory, file), chunksize=chunk_size)\n",
        "\n",
        "    # Initialize an empty list to store filtered chunks for this file\n",
        "    filtered_chunks = []\n",
        "\n",
        "    # Loop through the chunks in the current file\n",
        "    for chunk in chunks:\n",
        "        # Filter for 2023\n",
        "        chunk_2023 = chunk.loc[pd.to_datetime(chunk['starttime']).dt.year == 2023]\n",
        "\n",
        "        # Append the filtered chunk to the list\n",
        "        filtered_chunks.append(chunk_2023)\n",
        "\n",
        "    # Concatenate all filtered chunks of this file into one DataFrame\n",
        "    filtered_df = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "    # Append the filtered DataFrame of this file to the list\n",
        "    filtered_dfs.append(filtered_df)\n",
        "\n",
        "# Concatenate all filtered DataFrames from different files into one DataFrame\n",
        "tvod_2023 = pd.concat(filtered_dfs, ignore_index=True)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "tvod_2023.to_csv(\"tvod_2023.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "nwqzmG9MGE_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RATINGS 2023\n",
        "\n",
        "# Read the first chunk of the CSV file\n",
        "chunk_size = 100000  # Adjust the chunk size as needed\n",
        "rating_chunks = pd.read_csv(\"../rate.csv\", header=0, chunksize=chunk_size)\n",
        "\n",
        "# Initialize an empty list to store filtered chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "# Loop through the chunks\n",
        "for chunk in rating_chunks:\n",
        "    # Filter for 2023\n",
        "    rating_2023 = chunk.loc[pd.to_datetime(chunk['modtime']).dt.year == 2023]\n",
        "\n",
        "    # Convert modtime to datetime\n",
        "    rating_2023['modtime'] = pd.to_datetime(rating_2023['modtime'])\n",
        "\n",
        "    # Break up modtime into date and time columns\n",
        "    rating_2023['Date'] = rating_2023['modtime'].dt.date\n",
        "    rating_2023['Time'] = rating_2023['modtime'].dt.time\n",
        "\n",
        "    # Replace the created time pst column with a simpler version\n",
        "    rating_2023['date_created'] = pd.to_datetime(rating_2023['Date'])\n",
        "    rating_2023 = rating_2023.drop(['Date', 'modtime'], axis=1)\n",
        "\n",
        "    # Created the month, year, day, and quarter columns\n",
        "    rating_2023['year'] = rating_2023['date_created'].dt.year\n",
        "    rating_2023['month'] = rating_2023['date_created'].dt.month\n",
        "    rating_2023['day'] = rating_2023['date_created'].dt.day\n",
        "    rating_2023['quarter'] = rating_2023['date_created'].dt.quarter\n",
        "\n",
        "    # Append the filtered chunk to the list\n",
        "    filtered_chunks.append(rating_2023)\n",
        "\n",
        "# Concatenate all filtered chunks into one DataFrame\n",
        "rating_2023 = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "rating_2023.to_csv(\"rating_2023.csv\", index=False)"
      ],
      "metadata": {
        "id": "CqVp_46HGGkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# WISHLISTS 2023\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 100000  # Adjust the chunk size as needed\n",
        "\n",
        "# Read the chunks of the CSV file\n",
        "wishlist_chunks = pd.read_csv(\"../wish.csv\", header=0, chunksize=chunk_size)\n",
        "\n",
        "# Initialize an empty list to store filtered chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "# Loop through the chunks\n",
        "for chunk in wishlist_chunks:\n",
        "    # Filter for rows where either modtime or deltime is in 2023\n",
        "    wishlist_2023 = chunk.loc[(pd.to_datetime(chunk['modtime']).dt.year == 2023) |\n",
        "                             (pd.to_datetime(chunk['deltime']).dt.year == 2023)]\n",
        "\n",
        "    # Convert modtime and deltime to datetime\n",
        "    wishlist_2023['modtime'] = pd.to_datetime(wishlist_2023['modtime'])\n",
        "    wishlist_2023['deltime'] = pd.to_datetime(wishlist_2023['deltime'])\n",
        "\n",
        "    # Break up modtime and deltime into date and time columns\n",
        "    wishlist_2023['Mod Date'] = wishlist_2023['modtime'].dt.date\n",
        "    wishlist_2023['Mod Time'] = wishlist_2023['modtime'].dt.time\n",
        "    wishlist_2023['Del Date'] = wishlist_2023['deltime'].dt.date\n",
        "    wishlist_2023['Del Time'] = wishlist_2023['deltime'].dt.time\n",
        "\n",
        "    # Replace the modtime and deltime columns with simpler versions\n",
        "    wishlist_2023['date_modified'] = pd.to_datetime(wishlist_2023['Mod Date'])\n",
        "    wishlist_2023['date_deleted'] = pd.to_datetime(wishlist_2023['Del Date'])\n",
        "    wishlist_2023 = wishlist_2023.drop(['Mod Date', 'modtime', 'Del Date', 'deltime'], axis=1)\n",
        "\n",
        "    # Create the month, year, day, and quarter columns for modification date\n",
        "    wishlist_2023['mod_year'] = wishlist_2023['date_modified'].dt.year\n",
        "    wishlist_2023['mod_month'] = wishlist_2023['date_modified'].dt.month\n",
        "    wishlist_2023['mod_day'] = wishlist_2023['date_modified'].dt.day\n",
        "    wishlist_2023['mod_quarter'] = wishlist_2023['date_modified'].dt.quarter\n",
        "\n",
        "    # Create the month, year, day, and quarter columns for deletion date\n",
        "    wishlist_2023['del_year'] = wishlist_2023['date_deleted'].dt.year\n",
        "    wishlist_2023['del_month'] = wishlist_2023['date_deleted'].dt.month\n",
        "    wishlist_2023['del_day'] = wishlist_2023['date_deleted'].dt.day\n",
        "    wishlist_2023['del_quarter'] = wishlist_2023['date_deleted'].dt.quarter\n",
        "\n",
        "    # Append the filtered chunk to the list\n",
        "    filtered_chunks.append(wishlist_2023)\n",
        "\n",
        "# Concatenate all filtered chunks into one DataFrame\n",
        "wishlist_2023 = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "wishlist_2023.to_csv(\"wishlist_2023.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "RQeeRvHhleda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSACTIONS 2023\n",
        "\n",
        "# Read the first chunk of the CSV file\n",
        "chunk_size = 100000  # Adjust the chunk size as needed\n",
        "txns_chunks = pd.read_csv(\"../txns.csv\", header=0, chunksize=chunk_size)\n",
        "\n",
        "# Initialize an empty list to store filtered chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "# Loop through the chunks\n",
        "for chunk in txns_chunks:\n",
        "    # Filter for 2023\n",
        "    txns_2023 = chunk.loc[pd.to_datetime(chunk['purchdate']).dt.year == 2023]\n",
        "\n",
        "    # Convert purchtime to datetime\n",
        "    txns_2023['purchdate'] = pd.to_datetime(txns_2023['purchdate'])\n",
        "\n",
        "    # Break up purchtime into date and time columns\n",
        "    txns_2023['Date'] = txns_2023['purchdate'].dt.date\n",
        "    txns_2023['Time'] = txns_2023['purchdate'].dt.time\n",
        "\n",
        "    # Replace the created time pst column with a simpler version\n",
        "    txns_2023['date_created'] = pd.to_datetime(txns_2023['Date'])\n",
        "    txns_2023 = txns_2023.drop(['Date', 'purchdate'], axis=1)\n",
        "\n",
        "    # Created the month, year, day, and quarter columns\n",
        "    txns_2023['year'] = txns_2023['date_created'].dt.year\n",
        "    txns_2023['month'] = txns_2023['date_created'].dt.month\n",
        "    txns_2023['day'] = txns_2023['date_created'].dt.day\n",
        "    txns_2023['quarter'] = txns_2023['date_created'].dt.quarter\n",
        "\n",
        "    # Append the filtered chunk to the list\n",
        "    filtered_chunks.append(txns_2023)\n",
        "\n",
        "# Concatenate all filtered chunks into one DataFrame\n",
        "txns_2023 = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "txns_2023.to_csv(\"txns_2023.csv\", index=False)"
      ],
      "metadata": {
        "id": "wq1EmkwUGY_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to focus on AVOD streams in particular so we want to take the entire file and separate it into different csvs for different years for trend analysis"
      ],
      "metadata": {
        "id": "9gstTqbAGvHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to iterate through the entire avod file and print out each year so\n",
        "# we can make CSVs for each year\n",
        "\n",
        "chunk_size = 100000  # Adjust the chunk size as needed\n",
        "avod_chunks = pd.read_csv(\"../avod.csv\", header=0, chunksize=chunk_size)\n",
        "\n",
        "unique_years = set()  # To store unique years\n",
        "\n",
        "for chunk in avod_chunks:\n",
        "    unique_years.update(pd.to_datetime(chunk['starttime']).dt.year.unique())\n",
        "\n",
        "print(\"Unique years available in the file:\", sorted(unique_years))"
      ],
      "metadata": {
        "id": "uyC8Z4JUOlwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the range of years\n",
        "years = range(2016, 2023 + 1)  # 2016 to 2023\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 100000  # Adjust the chunk size as needed\n",
        "\n",
        "# Read the chunks of the file\n",
        "avod_chunks = pd.read_csv(\"../avod.csv\", header=0, chunksize=chunk_size)\n",
        "\n",
        "# Loop through each year\n",
        "for year in years:\n",
        "    # Initialize an empty list to store filtered chunks for the current year\n",
        "    filtered_chunks = []\n",
        "\n",
        "    # Loop through the chunks\n",
        "    for chunk in avod_chunks:\n",
        "        # Convert starttimepst to datetime\n",
        "        chunk['starttime'] = pd.to_datetime(chunk['starttime'])\n",
        "        # Filter for the current year\n",
        "        year_chunk = chunk[chunk['starttime'].dt.year == year]\n",
        "        # Append the filtered chunk to the list\n",
        "        filtered_chunks.append(year_chunk)\n",
        "\n",
        "    # Concatenate all filtered chunks into one DataFrame for the current year\n",
        "    avod_year = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "    # Save the DataFrame as a CSV file for the current year\n",
        "    avod_year.to_csv(f\"avod_{year}.csv\", index=False)\n",
        "\n",
        "    # Reset the iterator to read the CSV file again for the next year\n",
        "    avod_chunks = pd.read_csv(\"../avod.csv\", header=0, chunksize=chunk_size)\n"
      ],
      "metadata": {
        "id": "9GH7YtJNG6fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can begin feature engineering for each file, grouping by extid and by quarter"
      ],
      "metadata": {
        "id": "Mj6OVferQC18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read CSV file\n",
        "df = pd.read_csv(\"avod_2023.csv\")\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under 'platform' and 'quarter'\n",
        "pivot_df = df.pivot_table(index=['extid', 'quarter'], columns='platform', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'streams'\n",
        "stream_counts = df.groupby(['extid', 'quarter'])['streamsess'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'contents'\n",
        "content_counts = df.groupby(['extid', 'quarter'])['conts'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'platforms'\n",
        "platform_counts = df.groupby(['extid', 'quarter'])['plat'].nunique()\n",
        "\n",
        "# Concatenate the pivot_df with other aggregated results\n",
        "result_df = pd.concat([stream_counts, content_counts, platform_counts, pivot_df], axis=1)\n",
        "result_df.reset_index(inplace=True)\n",
        "\n",
        "# Renaming the columns\n",
        "result_df.columns = ['extid', 'quarter', 'stream_count', 'content_count','platform_counts'] + ['platform_' + str(col) for col in pivot_df.columns]\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "result_df.to_csv(\"result.csv\", index=False)\n",
        "\n",
        "print(\"Saved result_df as result.csv\")"
      ],
      "metadata": {
        "id": "4ubglgkTQNlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1= pd.read_csv(\"tvod_2023.csv\")\n",
        "df1['starttime'] = pd.to_datetime(df1['starttime'])\n",
        "df1['quarter'] = df1['starttime'].dt.quarter\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under 'platform' and 'quarter'\n",
        "pivot_df1 = df1.pivot_table(index=['extid', 'quarter'], columns='plat', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under 'videoquality' and 'quarter'\n",
        "pivot_df2 = df1.pivot_table(index=['extid', 'quarter'], columns='vide_qual', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under 'offer' and 'quarter'\n",
        "pivot_df3 = df1.pivot_table(index=['extid', 'quarter'], columns='off', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'streamingsessionid'\n",
        "stream_counts = df1.groupby(['extid', 'quarter'])['streamsess'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'contentid'\n",
        "content_counts = df1.groupby(['extid', 'quarter'])['cont'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'platform'\n",
        "platform_counts = df1.groupby(['extid', 'quarter'])['plat'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the occurrences of each 'quality'\n",
        "quality_counts = df1.groupby(['extid', 'quarter'])['videqual'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the occurrences of each 'offer'\n",
        "offer_counts = df1.groupby(['extid', 'quarter'])['off'].nunique()\n",
        "\n",
        "# Concatenate the pivot_df with other aggregated results\n",
        "result_df1 = pd.concat([stream_counts, content_counts, platform_counts, quality_counts, offer_counts, pivot_df1, pivot_df2, pivot_df3], axis=1)\n",
        "result_df1.reset_index(inplace=True)\n",
        "\n",
        "# Renaming the columns\n",
        "new_columns = ['extid', 'quarter', 'stream_count', 'content_count', 'platform_count', 'quality_count', 'offer_count'] \\\n",
        "              + ['platform_' + str(col) for col in pivot_df1.columns] \\\n",
        "              + ['quality_' + str(col) for col in pivot_df2.columns] \\\n",
        "              + ['offer_' + str(col) for col in pivot_df3.columns]\n",
        "\n",
        "result_df1.columns = new_columns\n",
        "\n",
        "print(result_df1)\n",
        "\n",
        "\n",
        "result_df1.to_csv(\"tvod_agg.csv\", index=False)\n",
        "\n",
        "print(\"Saved result_df as tvod_agg.csv\")\n"
      ],
      "metadata": {
        "id": "T7IW6o-4QR1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'contentid'\n",
        "content_counts = df3.groupby(['extid', 'quarter'])['cont'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and calculating the average star rating\n",
        "avg_rating = df3.groupby(['extid', 'quarter'])['stars'].mean()\n",
        "\n",
        "# Concatenate the pivot_df with other aggregated results\n",
        "result_df = pd.concat([content_counts, avg_rating], axis=1)\n",
        "result_df.reset_index(inplace=True)\n",
        "\n",
        "# Renaming the columns\n",
        "result_df.columns = ['extid', 'quarter', 'content_count','avg_rating']\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "\n",
        "result_df.to_csv(\"rating_agg.csv\", index=False)\n",
        "\n",
        "print(\"Saved result_df as rating_agg.csv\")"
      ],
      "metadata": {
        "id": "QoKEQX_QQT37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df2['date_mod'] = pd.to_datetime(df2['date_mod'])\n",
        "df2['quarter'] = df2['date_mod'].dt.quarter\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'modifications'\n",
        "mod = df2.groupby(['extid', 'quarter'])['Mod'].count()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and number of deletes\n",
        "delete = df2.groupby(['extid', 'quarter'])['Del'].count()\n",
        "\n",
        "# Concatenate the pivot_df with other aggregated results\n",
        "result_df = pd.concat([mod, delete], axis=1)\n",
        "result_df.reset_index(inplace=True)\n",
        "\n",
        "# Renaming the columns\n",
        "result_df.columns = ['extid', 'quarter', 'mod','delete']\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "\n",
        "result_df.to_csv(\"mod_agg.csv\", index=False)\n",
        "\n",
        "print(\"Saved result_df as mod_agg.csv\")"
      ],
      "metadata": {
        "id": "aMK32MRuQVd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pivot the DataFrame to create different columns for each category under offer\n",
        "pivot_df2 = df4.pivot_table(index=['extid', 'quarter'], columns='off', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under library\n",
        "pivot_df3 = df4.pivot_table(index=['extid', 'quarter'], columns='lib', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under purchase\n",
        "pivot_df4 = df4.pivot_table(index=['extid', 'quarter'], columns='purch', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under content\n",
        "pivot_df5 = df4.pivot_table(index=['extid', 'quarter'], columns='cont', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Pivot the DataFrame to create different columns for each category under videoquality\n",
        "pivot_df6 = df4.pivot_table(index=['extid', 'quarter'], columns='vidqual', aggfunc='size', fill_value=0)\n",
        "\n",
        "\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the number of unique 'contentid'\n",
        "content_counts = df4.groupby(['extid', 'quarter'])['cont'].nunique()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and total money spent\n",
        "totalmoney = df4.groupby(['extid', 'quarter'])['mon'].sum()\n",
        "\n",
        "# Grouping by 'extid' and 'quarter' and counting the occurrences of each 'purchase'\n",
        "purch_type = df4.groupby(['extid', 'quarter'])['purch'].nunique()\n",
        "\n",
        "# Concatenate the pivot_df with other aggregated results\n",
        "result_df1 = pd.concat([content_counts, totalmoney, purch_type,\n",
        "                        pivot_df2, pivot_df3, pivot_df4, pivot_df5, pivot_df6], axis=1)\n",
        "result_df1.reset_index(inplace=True)\n",
        "\n",
        "# Renaming the columns\n",
        "new_columns = ['extid', 'quarter', 'content_count','totalmoney','purchasetype'] \\\n",
        "              + ['txnoffer_' + str(col) for col in pivot_df2.columns] \\\n",
        "              + ['library_' + str(col) for col in pivot_df3.columns] \\\n",
        "              + ['purchase_' + str(col) for col in pivot_df4.columns] \\\n",
        "              + ['content_' + str(col) for col in pivot_df5.columns] \\\n",
        "              + ['video_' + str(col) for col in pivot_df6.columns]\n",
        "\n",
        "\n",
        "result_df1.columns = new_columns\n",
        "\n",
        "print(result_df1)\n",
        "\n",
        "\n",
        "result_df1.to_csv(\"half_txns.csv\", index=False)\n",
        "\n",
        "print(\"Saved result_df1 as half_txns.csv\")\n"
      ],
      "metadata": {
        "id": "Y5Ubihs_QXkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can combine our cleaned datasets to make a full/final dataset for our machine learning model"
      ],
      "metadata": {
        "id": "xoPj-CXKRYg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all files\n",
        "avod = pd.read_csv(\"avod_agg.csv\")\n",
        "tvod = pd.read_csv(\"tvod_agg.csv\")\n",
        "mod = pd.read_csv(\"mod_agg.csv\")\n",
        "rating = pd.read_csv(\"rating_agg.csv\")\n",
        "txns = pd.read_csv(\"half_txns.csv\")\n",
        "ref_cam = pd.read_csv(\"ref_cam.csv\")\n"
      ],
      "metadata": {
        "id": "0p3BVi3BRgdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature engineering/binning occured at every merge section. However could not include the code due to sensitive data"
      ],
      "metadata": {
        "id": "fgUfnehGjxQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine avod and tvod\n",
        "merged_df = pd.merge(avod, tvod, on=['extid', 'quarter'], suffixes=('_avod', '_tvod'), how= 'outer')\n",
        "merged_df = merged_df.fillna(0)\n",
        "merged_df.rename(columns={'stream_count_avod': 'avod','stream_count_tvod':'tvod'}, inplace=True)\n",
        "merged_df.to_csv(\"finalatvod.csv\", index=False)\n",
        "\n",
        "print(\"Saved as finalatvod.csv\")"
      ],
      "metadata": {
        "id": "n0ZqUvRBRiku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the modifications and ratings on 'extid' and 'quarter'\n",
        "lil_merge = pd.merge(mod, rating, on=['extid', 'quarter'], suffixes=('_mod', '_rat'), how= 'outer')\n",
        "lil_merge = lil_merge.fillna(0)\n",
        "\n",
        "lil_merge['edits'] = lil_merge['mod'] + lil_merge['delete']\n",
        "\n",
        "lil_merge.drop(columns=['mod','delete'], inplace=True)\n",
        "\n",
        "lil_merge.rename(columns={'content_count': '#contents_rated'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "lpTvH8-yRu2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge avod/tvod file and modifications/ratings file\n",
        "fourth = pd.read_csv(\"finalatvod.csv\")\n",
        "_merge = pd.merge(fourth, lil_merge, on=['extid', 'quarter'], suffixes=('_atvod', '_lil'), how = 'outer')\n",
        "_merge = _merge.fillna(0)\n",
        "_merge.to_csv(\"final_four_merge.csv\", index=False)\n",
        "\n",
        "print(\"Saved as final_four_merge.csv\")"
      ],
      "metadata": {
        "id": "o0Gv-_LaRv2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge transactions and referrer/campaigns\n",
        "full_txns = pd.merge(txns, ref_cam, on=['extid', 'quarter'], suffixes=('_txns', '_rc'), how='outer')\n",
        "full_txns = full_txns.fillna(0)\n",
        "full_txns.to_csv(\"final_txns.csv\", index=False)\n",
        "\n",
        "print(\"Saved as final_txns.csv\")"
      ],
      "metadata": {
        "id": "Ryha7Lv1R01P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four = pd.read_csv(\"final_four_merge.csv\")\n",
        "txns = pd.read_csv(\"final_txns.csv\")\n",
        "final = pd.merge(four, txns, on=['extid', 'quarter'], suffixes=('_four', '_txns'), how='outer')\n",
        "final = final.fillna(0)"
      ],
      "metadata": {
        "id": "tUO5eg7pR44-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = pd.merge(four, txns, on=['extid', 'quarter'], suffixes=('_four', '_txns'), how='outer')\n",
        "final = final.fillna(0)\n",
        "final.to_csv(\"final.csv\", index=False)\n",
        "\n",
        "print(\"Saved as final.csv\")"
      ],
      "metadata": {
        "id": "c4jjCz3eR-Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to find the account duration of each customer from the time of their account creation to the beginning of that quarter"
      ],
      "metadata": {
        "id": "Qt46mpp7SsOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.read_csv(\"../acc.csv\")"
      ],
      "metadata": {
        "id": "pyzgt594Sr8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all = pd.merge(final, acc, on='extid', how = 'left')"
      ],
      "metadata": {
        "id": "A0pkVA2FS0rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the 'acc_stand' column based on the 'quarter' column\n",
        "quarter_to_acc_stand = {\n",
        "    1: '01-01-2023',\n",
        "    2: '04-01-2023',\n",
        "    3: '07-01-2023',\n",
        "    4: '10-01-2023'\n",
        "}\n",
        "\n",
        "all['acc_stand'] = all['quarter'].map(quarter_to_acc_stand)\n",
        "\n",
        "\n",
        "all['acc_stand'] = pd.to_datetime(all['acc_stand'])"
      ],
      "metadata": {
        "id": "6x_B4oc4S1rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all['acc_stand'] = pd.to_datetime(all['acc_stand'])\n",
        "all['create_date'] = pd.to_datetime(all['createdtimepst'])\n",
        "\n",
        "# Subtract the two datetime columns to get the timedelta in days\n",
        "all['acc_dur'] = abs((all['acc_stand'] - all['create_date']).dt.days)\n",
        "\n",
        "all.drop(columns=['createdtimepst', 'acc_stand', 'create_date'], inplace=True)"
      ],
      "metadata": {
        "id": "PtnEB8ZsS3AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all.to_csv(\"ALL.csv\", index=False)"
      ],
      "metadata": {
        "id": "Bt8wkNuQS4f3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}